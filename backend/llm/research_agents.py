"""
Advanced Research LLM Agents for Novel Causal Discovery and Analysis.

This module implements cutting-edge research agents that can:
1. Discover novel causal relationships using LLM reasoning
2. Perform comparative causal inference studies  
3. Generate research hypotheses and experimental designs
4. Conduct automated literature reviews for causal methods
"""

import asyncio
import logging
import json
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass
from datetime import datetime
from abc import ABC, abstractmethod

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

from .llm_agents import BaseLLMAgent, LLMResponse, CausalQuery
from ..engine.causal_engine import CausalDAG, Intervention, CausalResult

logger = logging.getLogger(__name__)


@dataclass
class ResearchHypothesis:
    """Container for research hypothesis generated by LLM."""
    hypothesis_id: str
    title: str
    description: str
    variables_involved: List[str]
    predicted_relationships: Dict[str, str]
    testable_predictions: List[str]
    experimental_design: Dict[str, Any]
    novelty_score: float
    feasibility_score: float
    generated_by: str
    timestamp: datetime


@dataclass
class CausalDiscoveryResult:
    """Result of causal discovery process."""
    discovered_relationships: List[Tuple[str, str, float]]  # (source, target, strength)
    confidence_scores: Dict[Tuple[str, str], float]
    alternative_structures: List[CausalDAG]
    discovery_method: str
    evidence_summary: str
    limitations: List[str]
    next_experiments: List[str]


@dataclass
class ComparativeStudyResult:
    """Result of comparative causal inference study."""
    methods_compared: List[str]
    datasets_used: List[str]
    performance_metrics: Dict[str, Dict[str, float]]
    best_method: str
    method_rankings: List[Tuple[str, float]]
    contextual_recommendations: Dict[str, str]
    statistical_significance: Dict[str, bool]
    effect_size_comparisons: Dict[str, float]


class ResearchLLMAgent(BaseLLMAgent):
    """Advanced research agent for causal discovery and hypothesis generation."""
    
    def __init__(self, agent_id: str, model: str, research_domain: str = "causal_inference"):
        super().__init__(agent_id, model)
        self.research_domain = research_domain
        self.hypothesis_history: List[ResearchHypothesis] = []
        self.discovery_results: List[CausalDiscoveryResult] = []
        
    async def discover_causal_relationships(
        self,
        observational_data_description: str,
        domain_knowledge: Dict[str, Any],
        variables: List[str]
    ) -> CausalDiscoveryResult:
        """
        Use LLM reasoning to discover potential causal relationships in data.
        
        Args:
            observational_data_description: Description of available observational data
            domain_knowledge: Prior knowledge about the domain
            variables: List of variables in the dataset
            
        Returns:
            CausalDiscoveryResult with discovered relationships and evidence
        """
        start_time = datetime.now()
        
        prompt = self._build_discovery_prompt(
            observational_data_description, domain_knowledge, variables
        )
        
        try:
            if self.model.startswith("gpt"):
                response = await self._query_openai_for_discovery(prompt)
            else:
                response = await self._query_anthropic_for_discovery(prompt)
                
            discovered_relationships = self._extract_relationships(response.response)
            confidence_scores = self._extract_confidence_scores(response.response)
            alternative_structures = self._generate_alternative_dags(discovered_relationships, variables)
            
            result = CausalDiscoveryResult(
                discovered_relationships=discovered_relationships,
                confidence_scores=confidence_scores,
                alternative_structures=alternative_structures,
                discovery_method=f"llm_reasoning_{self.model}",
                evidence_summary=self._extract_evidence_summary(response.response),
                limitations=self._extract_limitations(response.response),
                next_experiments=self._extract_next_experiments(response.response)
            )
            
            self.discovery_results.append(result)
            return result
            
        except Exception as e:
            logger.error(f"Error in causal discovery: {e}")
            return CausalDiscoveryResult(
                discovered_relationships=[],
                confidence_scores={},
                alternative_structures=[],
                discovery_method=f"llm_reasoning_{self.model}",
                evidence_summary=f"Error occurred: {str(e)}",
                limitations=["LLM query failed"],
                next_experiments=[]
            )
    
    async def generate_research_hypothesis(
        self,
        domain: str,
        existing_literature: List[str],
        available_data: Dict[str, Any],
        research_gap: str
    ) -> ResearchHypothesis:
        """
        Generate novel research hypothesis based on literature gaps and available data.
        
        Args:
            domain: Research domain (e.g., "economics", "medicine", "psychology")
            existing_literature: List of relevant paper abstracts or summaries
            available_data: Description of available datasets
            research_gap: Identified gap in current research
            
        Returns:
            ResearchHypothesis with testable predictions and experimental design
        """
        start_time = datetime.now()
        
        prompt = f"""
        You are a research scientist specializing in causal inference and {domain}.
        
        Domain: {domain}
        
        Existing Literature Summary:
        {chr(10).join(existing_literature[:5])}  # Limit to first 5 for brevity
        
        Available Data:
        {json.dumps(available_data, indent=2)}
        
        Research Gap Identified:
        {research_gap}
        
        Task: Generate a novel, testable research hypothesis that:
        1. Addresses the identified research gap
        2. Can be tested with available data or feasible data collection
        3. Advances causal understanding in the domain
        4. Has clear practical implications
        
        Provide your hypothesis in this structured format:
        
        TITLE: [Concise title for the hypothesis]
        
        HYPOTHESIS: [Clear statement of the causal hypothesis]
        
        VARIABLES:
        - Treatment/Exposure: [main causal variable]
        - Outcome: [primary outcome variable]  
        - Mediators: [variables that mediate the effect]
        - Confounders: [potential confounding variables]
        - Moderators: [variables that moderate the effect]
        
        PREDICTED RELATIONSHIPS:
        - [Variable A] → [Variable B]: [Direction and expected strength]
        [Continue for all key relationships]
        
        TESTABLE PREDICTIONS:
        1. [Specific, measurable prediction]
        2. [Another specific prediction]
        [Continue with numbered predictions]
        
        EXPERIMENTAL DESIGN:
        Design Type: [RCT, Natural Experiment, Quasi-experimental, etc.]
        Sample Size: [Estimated required sample size]
        Data Collection: [Methods and timeline]
        Analysis Plan: [Statistical methods to be used]
        
        NOVELTY SCORE: [0-100 scale of how novel this hypothesis is]
        
        FEASIBILITY SCORE: [0-100 scale of how feasible this study is]
        
        PRACTICAL IMPLICATIONS: [Real-world applications of findings]
        """
        
        try:
            if self.model.startswith("gpt"):
                response = await self._query_openai_for_hypothesis(prompt)
            else:
                response = await self._query_anthropic_for_hypothesis(prompt)
                
            hypothesis = self._parse_hypothesis_response(response.response, domain, start_time)
            self.hypothesis_history.append(hypothesis)
            
            return hypothesis
            
        except Exception as e:
            logger.error(f"Error generating research hypothesis: {e}")
            return ResearchHypothesis(
                hypothesis_id=f"error_{start_time.isoformat()}",
                title="Error in hypothesis generation",
                description=f"Error occurred: {str(e)}",
                variables_involved=[],
                predicted_relationships={},
                testable_predictions=[],
                experimental_design={},
                novelty_score=0.0,
                feasibility_score=0.0,
                generated_by=self.agent_id,
                timestamp=start_time
            )
    
    async def conduct_comparative_study(
        self,
        methods_to_compare: List[str],
        evaluation_criteria: List[str],
        dataset_descriptions: List[str],
        ground_truth_effects: Optional[Dict[str, float]] = None
    ) -> ComparativeStudyResult:
        """
        Conduct a comparative study of different causal inference methods.
        
        Args:
            methods_to_compare: List of causal inference methods
            evaluation_criteria: Criteria for comparing methods
            dataset_descriptions: Descriptions of datasets for comparison
            ground_truth_effects: Known causal effects for validation
            
        Returns:
            ComparativeStudyResult with method performance and rankings
        """
        start_time = datetime.now()
        
        prompt = f"""
        You are conducting a systematic comparative study of causal inference methods.
        
        Methods to Compare:
        {chr(10).join([f"- {method}" for method in methods_to_compare])}
        
        Evaluation Criteria:
        {chr(10).join([f"- {criterion}" for criterion in evaluation_criteria])}
        
        Datasets:
        {chr(10).join([f"Dataset {i+1}: {desc}" for i, desc in enumerate(dataset_descriptions)])}
        
        Ground Truth Effects (if known):
        {json.dumps(ground_truth_effects, indent=2) if ground_truth_effects else "Not available"}
        
        Task: Provide a comprehensive comparative analysis including:
        
        1. THEORETICAL COMPARISON:
        For each method, analyze:
        - Assumptions required
        - Strengths and limitations  
        - Appropriate use cases
        - Computational complexity
        
        2. EXPECTED PERFORMANCE:
        For each dataset and criterion, predict performance of each method:
        - Accuracy/Bias
        - Precision/Variance
        - Robustness
        - Interpretability
        
        3. METHOD RANKINGS:
        Rank methods from best to worst for each:
        - Dataset type
        - Use case scenario
        - Overall recommendation
        
        4. CONTEXTUAL RECOMMENDATIONS:
        When to use each method based on:
        - Data characteristics
        - Research goals
        - Available resources
        
        5. STATISTICAL CONSIDERATIONS:
        - Power analysis
        - Multiple testing corrections
        - Effect size interpretation
        
        Format your response with clear sections and numerical scores (0-100) where applicable.
        """
        
        try:
            if self.model.startswith("gpt"):
                response = await self._query_openai_for_comparison(prompt)
            else:
                response = await self._query_anthropic_for_comparison(prompt)
                
            result = self._parse_comparative_study_response(
                response.response, methods_to_compare, dataset_descriptions
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Error in comparative study: {e}")
            return ComparativeStudyResult(
                methods_compared=methods_to_compare,
                datasets_used=dataset_descriptions,
                performance_metrics={},
                best_method="error",
                method_rankings=[],
                contextual_recommendations={},
                statistical_significance={},
                effect_size_comparisons={}
            )
    
    async def literature_review_synthesis(
        self,
        research_question: str,
        papers: List[Dict[str, Any]],
        synthesis_type: str = "systematic"
    ) -> Dict[str, Any]:
        """
        Conduct automated literature review and synthesis for causal research.
        
        Args:
            research_question: Specific research question to address
            papers: List of paper metadata (title, abstract, findings, etc.)
            synthesis_type: Type of synthesis ("systematic", "narrative", "meta-analytic")
            
        Returns:
            Dictionary with synthesis results including gaps and future directions
        """
        start_time = datetime.now()
        
        prompt = f"""
        Conduct a {synthesis_type} literature review synthesis on the following research question:
        
        Research Question: {research_question}
        
        Included Papers ({len(papers)} total):
        {self._format_papers_for_review(papers[:20])}  # Limit to first 20 for context
        
        Synthesis Task:
        1. EVIDENCE SYNTHESIS:
        - Consistent findings across studies
        - Contradictory or mixed results  
        - Methodological variations and their impact
        - Quality assessment of evidence
        
        2. CAUSAL MECHANISMS:
        - Proposed causal pathways
        - Mediating and moderating factors
        - Competing theoretical explanations
        
        3. METHODOLOGICAL ANALYSIS:
        - Research designs used
        - Strengths and limitations of methods
        - Identification strategies employed
        - Robustness of findings
        
        4. RESEARCH GAPS:
        - Understudied populations or contexts
        - Methodological limitations
        - Unexplored causal pathways
        - Missing control variables
        
        5. FUTURE DIRECTIONS:
        - High-priority research questions
        - Methodological innovations needed
        - Data collection recommendations
        - Policy implications
        
        6. QUANTITATIVE SYNTHESIS (if applicable):
        - Effect size estimates
        - Heterogeneity assessment
        - Publication bias analysis
        
        Provide specific citations and be precise about the strength of evidence for each conclusion.
        """
        
        try:
            if self.model.startswith("gpt"):
                response = await self._query_openai_for_review(prompt)
            else:
                response = await self._query_anthropic_for_review(prompt)
                
            synthesis = self._parse_literature_synthesis(response.response)
            
            return {
                "research_question": research_question,
                "synthesis_type": synthesis_type,
                "papers_reviewed": len(papers),
                "synthesis_results": synthesis,
                "generated_by": self.agent_id,
                "timestamp": start_time.isoformat(),
                "response_quality": self._assess_response_quality(response.response)
            }
            
        except Exception as e:
            logger.error(f"Error in literature synthesis: {e}")
            return {
                "research_question": research_question,
                "synthesis_type": synthesis_type,
                "papers_reviewed": len(papers),
                "synthesis_results": {"error": str(e)},
                "generated_by": self.agent_id,
                "timestamp": start_time.isoformat(),
                "response_quality": 0.0
            }
    
    def _build_discovery_prompt(
        self, 
        data_description: str, 
        domain_knowledge: Dict[str, Any], 
        variables: List[str]
    ) -> str:
        """Build prompt for causal discovery task."""
        return f"""
        You are an expert in causal discovery and data science. Your task is to identify 
        potential causal relationships in observational data using principled reasoning.
        
        Observational Data:
        {data_description}
        
        Variables Available:
        {', '.join(variables)}
        
        Domain Knowledge:
        {json.dumps(domain_knowledge, indent=2)}
        
        Task: Identify plausible causal relationships using the following approach:
        
        1. TEMPORAL REASONING:
        - Which variables could plausibly cause others based on timing?
        - What is the natural temporal ordering of events?
        
        2. MECHANISM ANALYSIS:
        - What are plausible causal mechanisms?
        - How could one variable physically/logically influence another?
        
        3. CONFOUNDING ASSESSMENT:
        - What common causes might exist?
        - Which relationships might be spurious?
        
        4. ALTERNATIVE EXPLANATIONS:
        - What alternative causal structures are possible?
        - How could we distinguish between them?
        
        Output Format:
        DISCOVERED RELATIONSHIPS:
        [Variable A] → [Variable B] (Confidence: X%, Strength: Y)
        [Explanation of mechanism and evidence]
        
        CONFIDENCE SCORES:
        [Relationship]: X% confidence based on [reasoning]
        
        ALTERNATIVE STRUCTURES:
        Structure 1: [Description]
        Structure 2: [Description]
        
        EVIDENCE SUMMARY:
        [Key evidence supporting discovered relationships]
        
        LIMITATIONS:
        [Limitations of the analysis and remaining uncertainties]
        
        NEXT EXPERIMENTS:
        [Suggested experiments to test causal hypotheses]
        """
    
    def _extract_relationships(self, response: str) -> List[Tuple[str, str, float]]:
        """Extract causal relationships from LLM response."""
        import re
        
        relationships = []
        # Look for patterns like "Variable A → Variable B (Confidence: 80%, Strength: 0.6)"
        pattern = r'(\w+(?:\s+\w+)*)\s*→\s*(\w+(?:\s+\w+)*)\s*\([^)]*Strength:\s*([0-9.]+)'
        
        matches = re.findall(pattern, response)
        for source, target, strength in matches:
            try:
                relationships.append((source.strip(), target.strip(), float(strength)))
            except ValueError:
                continue
                
        return relationships
    
    def _extract_confidence_scores(self, response: str) -> Dict[Tuple[str, str], float]:
        """Extract confidence scores for relationships."""
        import re
        
        confidence_scores = {}
        # Look for confidence patterns
        pattern = r'(\w+(?:\s+\w+)*)\s*→\s*(\w+(?:\s+\w+)*)[^:]*:\s*([0-9.]+)%'
        
        matches = re.findall(pattern, response)
        for source, target, confidence in matches:
            try:
                key = (source.strip(), target.strip())
                confidence_scores[key] = float(confidence) / 100.0
            except ValueError:
                continue
                
        return confidence_scores
    
    def _generate_alternative_dags(
        self, 
        relationships: List[Tuple[str, str, float]], 
        variables: List[str]
    ) -> List[CausalDAG]:
        """Generate alternative DAG structures."""
        # Simple implementation - could be enhanced with more sophisticated structure learning
        alternative_dags = []
        
        if relationships:
            # Create a DAG from discovered relationships
            edges = [(source, target) for source, target, _ in relationships]
            dag = CausalDAG(
                nodes=variables,
                edges=edges,
                node_data={var: [] for var in variables}
            )
            alternative_dags.append(dag)
            
        return alternative_dags
    
    def _extract_evidence_summary(self, response: str) -> str:
        """Extract evidence summary from response."""
        import re
        
        match = re.search(r'EVIDENCE SUMMARY:\s*(.+?)(?=LIMITATIONS:|$)', response, re.DOTALL)
        return match.group(1).strip() if match else "No evidence summary found"
    
    def _extract_limitations(self, response: str) -> List[str]:
        """Extract limitations from response."""
        import re
        
        match = re.search(r'LIMITATIONS:\s*(.+?)(?=NEXT EXPERIMENTS:|$)', response, re.DOTALL)
        if match:
            limitations_text = match.group(1).strip()
            # Split by bullet points or line breaks
            limitations = [lim.strip('- ').strip() for lim in limitations_text.split('\n') if lim.strip()]
            return limitations[:5]  # Limit to 5 limitations
        return []
    
    def _extract_next_experiments(self, response: str) -> List[str]:
        """Extract suggested next experiments."""
        import re
        
        match = re.search(r'NEXT EXPERIMENTS:\s*(.+?)$', response, re.DOTALL)
        if match:
            experiments_text = match.group(1).strip()
            experiments = [exp.strip('- ').strip() for exp in experiments_text.split('\n') if exp.strip()]
            return experiments[:5]  # Limit to 5 experiments
        return []
    
    async def _query_openai_for_discovery(self, prompt: str) -> LLMResponse:
        """Query OpenAI for causal discovery."""
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAI not available")
            
        client = openai.AsyncClient()
        start_time = datetime.now()
        
        response = await client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": "You are an expert causal inference researcher."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=2000
        )
        
        return LLMResponse(
            agent_id=self.agent_id,
            model=self.model,
            query=prompt,
            response=response.choices[0].message.content,
            reasoning_steps=[],
            predicted_effect=None,
            confidence=0.8,
            timestamp=start_time,
            processing_time=(datetime.now() - start_time).total_seconds(),
            metadata={"tokens_used": response.usage.total_tokens if response.usage else 0}
        )
    
    async def _query_anthropic_for_discovery(self, prompt: str) -> LLMResponse:
        """Query Anthropic for causal discovery."""
        if not ANTHROPIC_AVAILABLE:
            raise ImportError("Anthropic not available")
            
        client = anthropic.AsyncAnthropic()
        start_time = datetime.now()
        
        response = await client.messages.create(
            model=self.model,
            max_tokens=2000,
            temperature=0.3,
            messages=[{"role": "user", "content": prompt}]
        )
        
        return LLMResponse(
            agent_id=self.agent_id,
            model=self.model,
            query=prompt,
            response=response.content[0].text,
            reasoning_steps=[],
            predicted_effect=None,
            confidence=0.8,
            timestamp=start_time,
            processing_time=(datetime.now() - start_time).total_seconds(),
            metadata={"tokens_used": response.usage.input_tokens + response.usage.output_tokens}
        )
    
    def _parse_hypothesis_response(self, response: str, domain: str, timestamp: datetime) -> ResearchHypothesis:
        """Parse hypothesis generation response."""
        import re
        
        # Extract components using regex patterns
        title_match = re.search(r'TITLE:\s*(.+)', response)
        title = title_match.group(1).strip() if title_match else "Untitled Hypothesis"
        
        hypothesis_match = re.search(r'HYPOTHESIS:\s*(.+?)(?=VARIABLES:|$)', response, re.DOTALL)
        description = hypothesis_match.group(1).strip() if hypothesis_match else "No description"
        
        # Extract variables
        variables = []
        variables_match = re.search(r'VARIABLES:\s*(.+?)(?=PREDICTED RELATIONSHIPS:|$)', response, re.DOTALL)
        if variables_match:
            variables_text = variables_match.group(1)
            var_lines = [line.strip() for line in variables_text.split('\n') if line.strip() and ':' in line]
            for line in var_lines:
                if ':' in line:
                    var_name = line.split(':')[1].strip()
                    variables.append(var_name)
        
        # Extract predicted relationships
        relationships = {}
        rel_match = re.search(r'PREDICTED RELATIONSHIPS:\s*(.+?)(?=TESTABLE PREDICTIONS:|$)', response, re.DOTALL)
        if rel_match:
            rel_text = rel_match.group(1)
            rel_lines = [line.strip() for line in rel_text.split('\n') if line.strip() and '→' in line]
            for line in rel_lines:
                if '→' in line and ':' in line:
                    rel_part = line.split(':')[0].strip()
                    desc_part = line.split(':')[1].strip()
                    relationships[rel_part] = desc_part
        
        # Extract testable predictions
        predictions = []
        pred_match = re.search(r'TESTABLE PREDICTIONS:\s*(.+?)(?=EXPERIMENTAL DESIGN:|$)', response, re.DOTALL)
        if pred_match:
            pred_text = pred_match.group(1)
            pred_lines = [line.strip() for line in pred_text.split('\n') if line.strip()]
            predictions = [line.strip('1234567890. ').strip() for line in pred_lines if line.strip()][:5]
        
        # Extract experimental design
        design = {}
        design_match = re.search(r'EXPERIMENTAL DESIGN:\s*(.+?)(?=NOVELTY SCORE:|$)', response, re.DOTALL)
        if design_match:
            design_text = design_match.group(1)
            design_lines = [line.strip() for line in design_text.split('\n') if line.strip() and ':' in line]
            for line in design_lines:
                if ':' in line:
                    key, value = line.split(':', 1)
                    design[key.strip()] = value.strip()
        
        # Extract scores
        novelty_match = re.search(r'NOVELTY SCORE:\s*([0-9.]+)', response)
        novelty_score = float(novelty_match.group(1)) if novelty_match else 50.0
        
        feasibility_match = re.search(r'FEASIBILITY SCORE:\s*([0-9.]+)', response)
        feasibility_score = float(feasibility_match.group(1)) if feasibility_match else 50.0
        
        return ResearchHypothesis(
            hypothesis_id=f"{self.agent_id}_{timestamp.strftime('%Y%m%d_%H%M%S')}",
            title=title,
            description=description,
            variables_involved=variables,
            predicted_relationships=relationships,
            testable_predictions=predictions,
            experimental_design=design,
            novelty_score=min(100.0, max(0.0, novelty_score)),
            feasibility_score=min(100.0, max(0.0, feasibility_score)),
            generated_by=self.agent_id,
            timestamp=timestamp
        )
    
    async def _query_openai_for_hypothesis(self, prompt: str) -> LLMResponse:
        """Query OpenAI for hypothesis generation."""
        return await self._query_openai_for_discovery(prompt)  # Reuse discovery method
    
    async def _query_anthropic_for_hypothesis(self, prompt: str) -> LLMResponse:
        """Query Anthropic for hypothesis generation."""
        return await self._query_anthropic_for_discovery(prompt)  # Reuse discovery method
    
    async def _query_openai_for_comparison(self, prompt: str) -> LLMResponse:
        """Query OpenAI for comparative study."""
        return await self._query_openai_for_discovery(prompt)  # Reuse discovery method
    
    async def _query_anthropic_for_comparison(self, prompt: str) -> LLMResponse:
        """Query Anthropic for comparative study."""
        return await self._query_anthropic_for_discovery(prompt)  # Reuse discovery method
    
    async def _query_openai_for_review(self, prompt: str) -> LLMResponse:
        """Query OpenAI for literature review."""
        return await self._query_openai_for_discovery(prompt)  # Reuse discovery method
    
    async def _query_anthropic_for_review(self, prompt: str) -> LLMResponse:
        """Query Anthropic for literature review."""
        return await self._query_anthropic_for_discovery(prompt)  # Reuse discovery method
    
    def _parse_comparative_study_response(
        self, 
        response: str, 
        methods: List[str], 
        datasets: List[str]
    ) -> ComparativeStudyResult:
        """Parse comparative study response."""
        import re
        
        # Extract performance metrics (simplified parsing)
        performance_metrics = {}
        for method in methods:
            performance_metrics[method] = {
                "accuracy": 75.0,  # Default values - would be extracted from response
                "precision": 70.0,
                "robustness": 80.0,
                "interpretability": 65.0
            }
        
        # Extract method rankings
        rankings = [(method, 75.0) for method in methods]  # Simplified
        
        # Extract contextual recommendations
        recommendations = {
            "large_samples": methods[0] if methods else "none",
            "small_samples": methods[-1] if methods else "none",
            "high_dimensionality": methods[0] if methods else "none"
        }
        
        return ComparativeStudyResult(
            methods_compared=methods,
            datasets_used=datasets,
            performance_metrics=performance_metrics,
            best_method=methods[0] if methods else "none",
            method_rankings=rankings,
            contextual_recommendations=recommendations,
            statistical_significance={method: True for method in methods},
            effect_size_comparisons={method: 0.5 for method in methods}
        )
    
    def _format_papers_for_review(self, papers: List[Dict[str, Any]]) -> str:
        """Format papers for literature review prompt."""
        formatted = []
        for i, paper in enumerate(papers[:10]):  # Limit to 10 papers
            title = paper.get("title", f"Paper {i+1}")
            abstract = paper.get("abstract", "No abstract available")[:500]  # Truncate
            formatted.append(f"Paper {i+1}: {title}\nAbstract: {abstract}\n")
        
        return "\n".join(formatted)
    
    def _parse_literature_synthesis(self, response: str) -> Dict[str, Any]:
        """Parse literature synthesis response."""
        import re
        
        # Extract main sections
        synthesis = {
            "evidence_synthesis": self._extract_section(response, "EVIDENCE SYNTHESIS"),
            "causal_mechanisms": self._extract_section(response, "CAUSAL MECHANISMS"),
            "methodological_analysis": self._extract_section(response, "METHODOLOGICAL ANALYSIS"),
            "research_gaps": self._extract_section(response, "RESEARCH GAPS"),
            "future_directions": self._extract_section(response, "FUTURE DIRECTIONS"),
            "quantitative_synthesis": self._extract_section(response, "QUANTITATIVE SYNTHESIS")
        }
        
        return synthesis
    
    def _extract_section(self, response: str, section_name: str) -> str:
        """Extract a specific section from response."""
        import re
        
        pattern = rf'{section_name}:\s*(.+?)(?=[A-Z\s]+:|$)'
        match = re.search(pattern, response, re.DOTALL)
        return match.group(1).strip() if match else f"No {section_name.lower()} found"
    
    def _assess_response_quality(self, response: str) -> float:
        """Assess the quality of LLM response."""
        # Simple quality assessment based on length and structure
        word_count = len(response.split())
        has_structure = any(keyword in response.upper() for keyword in 
                          ["EVIDENCE", "ANALYSIS", "CONCLUSIONS", "LIMITATIONS"])
        
        quality_score = min(100.0, (word_count / 10) + (30 if has_structure else 0))
        return quality_score / 100.0